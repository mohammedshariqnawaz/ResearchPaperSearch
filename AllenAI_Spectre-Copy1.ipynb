{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:25:08.172377Z",
     "iopub.status.busy": "2021-08-21T09:25:08.172062Z",
     "iopub.status.idle": "2021-08-21T09:25:14.122947Z",
     "shell.execute_reply": "2021-08-21T09:25:14.122024Z",
     "shell.execute_reply.started": "2021-08-21T09:25:08.172349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "Collecting tokenizers>=0.10.3\n",
      "  Using cached tokenizers-0.10.3-cp38-cp38-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\python38\\lib\\site-packages (from sentence_transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\python38\\lib\\site-packages (from sentence_transformers) (1.9.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.10.1-cp38-cp38-win_amd64.whl (936 kB)\n",
      "Requirement already satisfied: numpy in c:\\python38\\lib\\site-packages (from sentence_transformers) (1.20.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.7.1-cp38-cp38-win_amd64.whl (33.7 MB)\n",
      "Requirement already satisfied: nltk in c:\\users\\mohds\\appdata\\roaming\\python\\python38\\site-packages (from sentence_transformers) (3.6.5)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\python38\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in c:\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mohds\\appdata\\roaming\\python\\python38\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (20.9)\n",
      "Requirement already satisfied: requests in c:\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2.26.0)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python38\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\python38\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\mohds\\appdata\\roaming\\python\\python38\\site-packages (from tqdm->sentence_transformers) (0.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\mohds\\appdata\\roaming\\python\\python38\\site-packages (from nltk->sentence_transformers) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\mohds\\appdata\\roaming\\python\\python38\\site-packages (from nltk->sentence_transformers) (8.0.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python38\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.26.7)\n",
      "Requirement already satisfied: six in c:\\python38\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence_transformers) (1.15.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pillow>=5.3.0 in c:\\python38\\lib\\site-packages (from torchvision->sentence_transformers) (8.2.0)\n",
      "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
      "Installing collected packages: tokenizers, threadpoolctl, scipy, sacremoses, huggingface-hub, transformers, torchvision, sentencepiece, scikit-learn, sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\python38\\\\Scripts\\\\sacremoses.exe' -> 'c:\\\\python38\\\\Scripts\\\\sacremoses.exe.deleteme'\n",
      "\n",
      "WARNING: You are using pip version 21.0.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the 'c:\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-af40dbb9f739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install sentence_transformers'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "!pip install sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:54:54.437431Z",
     "iopub.status.busy": "2021-08-21T07:54:54.437085Z",
     "iopub.status.idle": "2021-08-21T07:54:58.422320Z",
     "shell.execute_reply": "2021-08-21T07:54:58.421337Z",
     "shell.execute_reply.started": "2021-08-21T07:54:54.437392Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 147937477/3140261595 [00:03<01:20, 37278937.28it/s]\n"
     ]
    }
   ],
   "source": [
    "input_file = \"../input/arxiv/arxiv-metadata-oai-snapshot.json\"\n",
    "data  = []\n",
    "i = 0\n",
    "with tqdm.tqdm(total=os.path.getsize(input_file)) as pbar:\n",
    "     with open(input_file, 'r') as f:\n",
    "            \n",
    "        for line in f:\n",
    "            pbar.update(len(line))\n",
    "            data.append(json.loads(line))\n",
    "            i += 1\n",
    "            if i == 100000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:54:58.424638Z",
     "iopub.status.busy": "2021-08-21T07:54:58.424282Z",
     "iopub.status.idle": "2021-08-21T07:54:58.915588Z",
     "shell.execute_reply": "2021-08-21T07:54:58.914677Z",
     "shell.execute_reply.started": "2021-08-21T07:54:58.424599Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6a3aab1919cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpapers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "papers = pd.DataFrame(data)\n",
    "n = len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:54:58.917757Z",
     "iopub.status.busy": "2021-08-21T07:54:58.917377Z",
     "iopub.status.idle": "2021-08-21T07:54:58.955310Z",
     "shell.execute_reply": "2021-08-21T07:54:58.954387Z",
     "shell.execute_reply.started": "2021-08-21T07:54:58.917702Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'papers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-50f6006ac438>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'papers' is not defined"
     ]
    }
   ],
   "source": [
    "papers.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:54:58.956980Z",
     "iopub.status.busy": "2021-08-21T07:54:58.956598Z",
     "iopub.status.idle": "2021-08-21T07:55:04.813738Z",
     "shell.execute_reply": "2021-08-21T07:55:04.812898Z",
     "shell.execute_reply.started": "2021-08-21T07:54:58.956944Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1806530e1b2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpaper_texts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpaper_texts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'[SEP]'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n' is not defined"
     ]
    }
   ],
   "source": [
    "paper_texts = [None for i in range(n)]\n",
    "\n",
    "for i in range(n):\n",
    "    paper_texts[i] = papers.iloc[i, 3] + '[SEP]' + papers.iloc[i, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T08:52:33.502563Z",
     "iopub.status.busy": "2021-08-21T08:52:33.502252Z",
     "iopub.status.idle": "2021-08-21T08:52:33.549291Z",
     "shell.execute_reply": "2021-08-21T08:52:33.548478Z",
     "shell.execute_reply.started": "2021-08-21T08:52:33.502534Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paper_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-56fcf1f29110>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpapers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Input_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaper_texts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'paper_texts' is not defined"
     ]
    }
   ],
   "source": [
    "papers['Input_text'] = paper_texts\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paper_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f44cf14ec20d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaper_texts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'paper_texts' is not defined"
     ]
    }
   ],
   "source": [
    "print(paper_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the allenAI Specter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:55:04.873590Z",
     "iopub.status.busy": "2021-08-21T07:55:04.873283Z",
     "iopub.status.idle": "2021-08-21T07:55:41.119726Z",
     "shell.execute_reply": "2021-08-21T07:55:41.118810Z",
     "shell.execute_reply.started": "2021-08-21T07:55:04.873561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9768675a6e499cba67fde9d01a4e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ba10d6ac824e1c811eaadc079a25e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081b5f3c16c64870b72f3bc832f07d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a039c5ab2d764af9a5ad8d954f7f77bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5635ca15efdd48c295f086ccd30b7444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7932ecb6aebe403b9157a186ef8ac815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73f8f6a667418c89b7d81ceed2b440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e69e56dc64f4b8ba9d2f5305cf6891d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4dba577b1f5499b86ca48ff0c31d79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/462k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dfdf9ff35b49c58ac40268da0b6337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/331 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927ce7ddf39240718d7dc297c1b5ea59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c1408130c342a494ceb1ca62ff8935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('allenai-specter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T07:55:41.122217Z",
     "iopub.status.busy": "2021-08-21T07:55:41.121947Z",
     "iopub.status.idle": "2021-08-21T08:10:53.621596Z",
     "shell.execute_reply": "2021-08-21T08:10:53.620520Z",
     "shell.execute_reply.started": "2021-08-21T07:55:41.122191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90892af62e4d6eb657aba885dd8bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = model.encode(paper_texts, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T08:11:55.972830Z",
     "iopub.status.busy": "2021-08-21T08:11:55.972382Z",
     "iopub.status.idle": "2021-08-21T08:11:56.900629Z",
     "shell.execute_reply": "2021-08-21T08:11:56.899616Z",
     "shell.execute_reply.started": "2021-08-21T08:11:55.972788Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('embeddings_100k', 'wb') as f:\n",
    "    pickle.dump(corpus_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining search paper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:14:43.088489Z",
     "iopub.status.busy": "2021-08-21T09:14:43.088164Z",
     "iopub.status.idle": "2021-08-21T09:14:43.096851Z",
     "shell.execute_reply": "2021-08-21T09:14:43.095693Z",
     "shell.execute_reply.started": "2021-08-21T09:14:43.088449Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_papers(title, abstract):\n",
    "    query_embedding = model.encode(title+'[SEP]'+abstract, convert_to_tensor=True)\n",
    "\n",
    "    search_hits = util.semantic_search(query_embedding, corpus_embeddings)\n",
    "    search_hits = search_hits[0]  #Get the hits for the first query\n",
    "\n",
    "#   print(\"\\n\\nPaper:\", title)\n",
    "#   print(\"Most similar papers:\")\n",
    "#   for hit in search_hits:\n",
    "#         try:\n",
    "#             related_paper = papers[hit['corpus_id']]\n",
    "#             print(\"{:.2f}\\t{}\\t{} {}\".format(hit['score'], related_paper['title'], related_paper['venue'], related_paper['year']))\n",
    "#         except(KeyError):\n",
    "#             print(\"keyerror\")\n",
    "    search_hits = pd.DataFrame(search_hits)\n",
    "#     print(search_hits)\n",
    "    final_result = pd.DataFrame()\n",
    "    title = list()\n",
    "    authors = list()\n",
    "    arxiv_id = list()\n",
    "    score = list()\n",
    "    for i in range(len(search_hits)):\n",
    "        num = search_hits.iloc[i, 0]\n",
    "        title.append(papers.iloc[num, 3])\n",
    "        authors.append(papers.iloc[num, 2])\n",
    "        arxiv_id.append(papers.iloc[num, 0])\n",
    "        score.append(search_hits.iloc[i, 1])\n",
    "        \n",
    "    final_result['title'] = title\n",
    "    final_result['authors'] = authors\n",
    "    final_result['arxiv_id'] = arxiv_id\n",
    "    final_result['score'] = score\n",
    "    \n",
    "    display(HTML(final_result.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:14:47.468206Z",
     "iopub.status.busy": "2021-08-21T09:14:47.467787Z",
     "iopub.status.idle": "2021-08-21T09:14:47.595077Z",
     "shell.execute_reply": "2021-08-21T09:14:47.593885Z",
     "shell.execute_reply.started": "2021-08-21T09:14:47.468170Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0ab6c329c94424a50fc9bd9eb4cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EasyVoice: Integrating voice synthesis with Skype</td>\n",
       "      <td>Paulo A. Condado and Fernando G. Lobo</td>\n",
       "      <td>0706.3132</td>\n",
       "      <td>0.818029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Am\\'elioration des Performances des Syst\\`emes Automatiques de\\n  Reconnaissance de la Parole pour la Parole Non Native</td>\n",
       "      <td>Ghazi Bouselmi (INRIA Lorraine - LORIA), Dominique Fohr (INRIA\\n  Lorraine - LORIA), Irina Illina (INRIA Lorraine - LORIA), Jean-Paul Haton\\n  (INRIA Lorraine - LORIA)</td>\n",
       "      <td>0711.1038</td>\n",
       "      <td>0.807498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adjusted Viterbi training for hidden Markov models</td>\n",
       "      <td>J. Lember, A. Koloydenko</td>\n",
       "      <td>0709.2317</td>\n",
       "      <td>0.770014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice\\n  Disorder Detection</td>\n",
       "      <td>Max A Little, Patrick E McSharry, Stephen J Roberts, Declan AE\\n  Costello and Irene M Moroz</td>\n",
       "      <td>0707.0086</td>\n",
       "      <td>0.766409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel\\n  Recognition in Continues Speech</td>\n",
       "      <td>Mohammad Nazari, Abolghasem Sayadiyan, SeyedMajid Valiollahzadeh</td>\n",
       "      <td>0812.2411</td>\n",
       "      <td>0.761897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Acoustic Features and Perceptive Cues of Songs and Dialogues in Whistled\\n  Speech: Convergences with Sung Speech</td>\n",
       "      <td>Julien Meyer (LAB-Upc)</td>\n",
       "      <td>0711.3704</td>\n",
       "      <td>0.760309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Combined Acoustic and Pronunciation Modelling for Non-Native Speech\\n  Recognition</td>\n",
       "      <td>Ghazi Bouselmi (INRIA Lorraine - LORIA), Dominique Fohr (INRIA\\n  Lorraine - LORIA), Irina Illina (INRIA Lorraine - LORIA)</td>\n",
       "      <td>0711.0811</td>\n",
       "      <td>0.756685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Phoneme recognition in TIMIT with BLSTM-CTC</td>\n",
       "      <td>Santiago Fern\\'andez, Alex Graves, Juergen Schmidhuber</td>\n",
       "      <td>0804.3269</td>\n",
       "      <td>0.753250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A biomechanical model of the face including muscles for the prediction\\n  of deformations during speech production</td>\n",
       "      <td>Julie Groleau (TIMC), Matthieu Chabanas (TIMC), Christophe Marecaux\\n  (TIMC), Natacha Payrard, Brice Segaud, Michel Rochette, Pascal Perrier (ICP),\\n  Yohan Payan (TIMC)</td>\n",
       "      <td>0803.3924</td>\n",
       "      <td>0.750757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Suppl\\'eance perceptive par \\'electro-stimulation linguale embarqu\\'ee :\\n  perspectives pour la pr\\'evention des escarres chez le bless\\'e m\\'edullaire</td>\n",
       "      <td>Olivier Chenu (TIMC), Nicolas Vuillerme (TIMC), Alexandre\\n  Moreau-Gaudry (TIMC), Anthony Fleury (TIMC), Jacques Demongeot (TIMC), Yohan\\n  Payan (TIMC)</td>\n",
       "      <td>0711.3786</td>\n",
       "      <td>0.746030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This paper was the EMNLP 2020 Best Paper\n",
    "search_papers(title='Digital Voicing of Silent Speech',\n",
    "              abstract='In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:15:27.678388Z",
     "iopub.status.busy": "2021-08-21T09:15:27.678066Z",
     "iopub.status.idle": "2021-08-21T09:15:27.757389Z",
     "shell.execute_reply": "2021-08-21T09:15:27.756443Z",
     "shell.execute_reply.started": "2021-08-21T09:15:27.678360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308587be552f4469829ddc79b99b2772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infinite Viterbi alignments in the two state hidden Markov models</td>\n",
       "      <td>J. Lember, A. Koloydenko</td>\n",
       "      <td>0711.0928</td>\n",
       "      <td>0.819104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On the Vocabulary of Grammar-Based Codes and the Logical Consistency of\\n  Texts</td>\n",
       "      <td>{\\L}ukasz D\\k{e}bowski</td>\n",
       "      <td>0810.3125</td>\n",
       "      <td>0.814937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLAIRLIB Documentation v1.03</td>\n",
       "      <td>Dragomir Radev, Mark Hodges, Anthony Fader, Mark Joseph, Joshua\\n  Gerrish, Mark Schaller, Jonathan dePeri, Bryan Gibson</td>\n",
       "      <td>0712.3298</td>\n",
       "      <td>0.806504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Questions &amp; Answers for TEI Newcomers</td>\n",
       "      <td>Laurent Romary (LORIA)</td>\n",
       "      <td>0812.3563</td>\n",
       "      <td>0.803840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The emerging field of language dynamics</td>\n",
       "      <td>S. Wichmann</td>\n",
       "      <td>0801.1415</td>\n",
       "      <td>0.799843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In memoriam Maurice Gross</td>\n",
       "      <td>Eric Laporte (IGM-LabInfo)</td>\n",
       "      <td>0711.3452</td>\n",
       "      <td>0.799252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A constructive proof of the existence of Viterbi processes</td>\n",
       "      <td>J. Lember, A. Koloydenko</td>\n",
       "      <td>0804.2138</td>\n",
       "      <td>0.798086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Morphic and Automatic Words: Maximal Blocks and Diophantine\\n  Approximation</td>\n",
       "      <td>Yann Bugeaud, Dalia Krieger, Jeffrey Shallit</td>\n",
       "      <td>0808.2544</td>\n",
       "      <td>0.795205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Three Lectures on Automatic Structures</td>\n",
       "      <td>Bakhadyr Khoussainov, Mia Minnes</td>\n",
       "      <td>0809.3430</td>\n",
       "      <td>0.789193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UNL-French deconversion as transfer &amp; generation from an interlingua\\n  with possible quality enhancement through offline human interaction</td>\n",
       "      <td>Gilles s\\'erasset (IMAG, Clips - Imag, Lig), Christian Boitet (IMAG,\\n  Clips - Imag, Lig)</td>\n",
       "      <td>0811.0579</td>\n",
       "      <td>0.785540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This paper was a EMNLP 2020 Honourable Mention Papers\n",
    "search_papers(title='If beam search is the answer, what was the question?',\n",
    "              abstract='Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:15:37.123113Z",
     "iopub.status.busy": "2021-08-21T09:15:37.122798Z",
     "iopub.status.idle": "2021-08-21T09:15:37.199119Z",
     "shell.execute_reply": "2021-08-21T09:15:37.198181Z",
     "shell.execute_reply.started": "2021-08-21T09:15:37.123086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18618cc7a88942a7b5cc9c93b4f74fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Goal-oriented Dialog as a Collaborative Subordinated Activity involving\\n  Collective Acceptance</td>\n",
       "      <td>Sylvie Saget (IRISA), Marc Guyomard (IRISA)</td>\n",
       "      <td>0805.4101</td>\n",
       "      <td>0.801561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLAIRLIB Documentation v1.03</td>\n",
       "      <td>Dragomir Radev, Mark Hodges, Anthony Fader, Mark Joseph, Joshua\\n  Gerrish, Mark Schaller, Jonathan dePeri, Bryan Gibson</td>\n",
       "      <td>0712.3298</td>\n",
       "      <td>0.794038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Plate-forme Magicien d'Oz pour l'\\'etude de l'apport des ACAs \\`a\\n  l'interaction</td>\n",
       "      <td>J\\'er\\^ome Simonin (INRIA Rocquencourt / INRIA Lorraine - LORIA),\\n  Marius Hategan (INRIA Rocquencourt / INRIA Lorraine - LORIA), No\\\"elle\\n  Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)</td>\n",
       "      <td>0708.3740</td>\n",
       "      <td>0.789749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Optimizing Scrip Systems: Efficiency, Crashes, Hoarders, and Altruists</td>\n",
       "      <td>Ian A. Kash, Eric J. Friedman, Joseph Y. Halpern</td>\n",
       "      <td>0705.4110</td>\n",
       "      <td>0.770145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Human-Machine Symbiosis, 50 Years On</td>\n",
       "      <td>Ian Foster</td>\n",
       "      <td>0712.2255</td>\n",
       "      <td>0.767971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The noisy veto-voter model: a Recursive Distributional Equation on [0,1]</td>\n",
       "      <td>Saul Jacka and Marcus Sheehan</td>\n",
       "      <td>0804.3943</td>\n",
       "      <td>0.763506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SimDialog: A visual game dialog editor</td>\n",
       "      <td>C. Owen, F. Biocca, C. Bohil, J. Conley</td>\n",
       "      <td>0804.4885</td>\n",
       "      <td>0.762032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Infinite Viterbi alignments in the two state hidden Markov models</td>\n",
       "      <td>J. Lember, A. Koloydenko</td>\n",
       "      <td>0711.0928</td>\n",
       "      <td>0.760852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Comment: Expert Elicitation for Reliable System Design</td>\n",
       "      <td>Norman Fenton, Martin Neil</td>\n",
       "      <td>0708.0285</td>\n",
       "      <td>0.760090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Comment: Expert Elicitation for Reliable System Design</td>\n",
       "      <td>Andrew Koehler</td>\n",
       "      <td>0708.0287</td>\n",
       "      <td>0.760090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This paper was a EMNLP 2020 Honourable Mention Papers\n",
    "search_papers(title='Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems',\n",
    "              abstract='The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots). Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis. This metric has the ability to correlate a bot‚Äôs performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results. The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work. The framework is released asa ready-to-use tool.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:15:47.968233Z",
     "iopub.status.busy": "2021-08-21T09:15:47.967902Z",
     "iopub.status.idle": "2021-08-21T09:15:48.045870Z",
     "shell.execute_reply": "2021-08-21T09:15:48.044936Z",
     "shell.execute_reply.started": "2021-08-21T09:15:47.968206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8655bf6ad616404789b372d9eb546f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In memoriam Maurice Gross</td>\n",
       "      <td>Eric Laporte (IGM-LabInfo)</td>\n",
       "      <td>0711.3452</td>\n",
       "      <td>0.784882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Open architecture for multilingual parallel texts</td>\n",
       "      <td>M.T. Carrasco Benitez</td>\n",
       "      <td>0808.3889</td>\n",
       "      <td>0.771049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bootstrapping Deep Lexical Resources: Resources for Courses</td>\n",
       "      <td>Timothy Baldwin</td>\n",
       "      <td>0709.2401</td>\n",
       "      <td>0.766714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Methods to integrate a language model with semantic information for a\\n  word prediction component</td>\n",
       "      <td>Tonio Wandmacher, Jean-Yves Antoine</td>\n",
       "      <td>0801.4716</td>\n",
       "      <td>0.752721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Outilex, plate-forme logicielle de traitement de textes \\'ecrits</td>\n",
       "      <td>Olivier Blanc (IGM-LabInfo), Matthieu Constant (IGM-LabInfo), Eric\\n  Laporte (IGM-LabInfo)</td>\n",
       "      <td>0711.3691</td>\n",
       "      <td>0.745125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Commonsense Knowledge, Ontology and Ordinary Language</td>\n",
       "      <td>Walid S. Saba</td>\n",
       "      <td>0808.1211</td>\n",
       "      <td>0.743726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>International Standard for a Linguistic Annotation Framework</td>\n",
       "      <td>Laurent Romary (INRIA Lorraine - LORIA), Nancy Ide (INRIA Lorraine -\\n  LORIA)</td>\n",
       "      <td>0707.3269</td>\n",
       "      <td>0.743712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The VO-Neural project: recent developments and some applications</td>\n",
       "      <td>M. Brescia, S. Cavuoti, G. d'Angelo, R. D'Abrusco, N. Deniskina, M.\\n  Garofalo, O. Laurino, G. Longo, A. Nocella, B. Skordovski</td>\n",
       "      <td>0806.1006</td>\n",
       "      <td>0.738122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Constructing word similarities in Meroitic as an aid to decipherment</td>\n",
       "      <td>Reginald D. Smith</td>\n",
       "      <td>0808.3616</td>\n",
       "      <td>0.737939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>About the creation of a parallel bilingual corpora of web-publications</td>\n",
       "      <td>D.V. Lande and V.V. Zhygalo</td>\n",
       "      <td>0807.0311</td>\n",
       "      <td>0.736714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EMNLP 2020 paper on making Sentence-BERT multilingual\n",
    "search_papers(title='Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation',\n",
    "              abstract='We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-21T09:24:50.752874Z",
     "iopub.status.busy": "2021-08-21T09:24:50.752546Z",
     "iopub.status.idle": "2021-08-21T09:24:50.830407Z",
     "shell.execute_reply": "2021-08-21T09:24:50.829510Z",
     "shell.execute_reply.started": "2021-08-21T09:24:50.752845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639a47e1f2b54703a2f9c49d3ed2c3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Methods to integrate a language model with semantic information for a\\n  word prediction component</td>\n",
       "      <td>Tonio Wandmacher, Jean-Yves Antoine</td>\n",
       "      <td>0801.4716</td>\n",
       "      <td>0.826351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bit-Optimal Lempel-Ziv compression</td>\n",
       "      <td>Paolo Ferragina, Igor Nitto and Rossano Venturini</td>\n",
       "      <td>0802.0835</td>\n",
       "      <td>0.798463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In memoriam Maurice Gross</td>\n",
       "      <td>Eric Laporte (IGM-LabInfo)</td>\n",
       "      <td>0711.3452</td>\n",
       "      <td>0.794045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Determining the Unithood of Word Sequences using a Probabilistic\\n  Approach</td>\n",
       "      <td>Wilson Wong, Wei Liu, Mohammed Bennamoun</td>\n",
       "      <td>0810.0139</td>\n",
       "      <td>0.786225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bootstrapping Deep Lexical Resources: Resources for Courses</td>\n",
       "      <td>Timothy Baldwin</td>\n",
       "      <td>0709.2401</td>\n",
       "      <td>0.785482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word-Based Text Compression</td>\n",
       "      <td>Jan Platos, Jiri Dvorsky</td>\n",
       "      <td>0804.3680</td>\n",
       "      <td>0.784538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Determining the Unithood of Word Sequences using Mutual Information and\\n  Independence Measure</td>\n",
       "      <td>Wilson Wong, Wei Liu, Mohammed Bennamoun</td>\n",
       "      <td>0810.0156</td>\n",
       "      <td>0.782135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A Formal Model of Dictionary Structure and Content</td>\n",
       "      <td>Laurent Romary (INRIA Lorraine - LORIA), Nancy Ide, Adam Kilgarriff</td>\n",
       "      <td>0707.3270</td>\n",
       "      <td>0.779422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pushdown Compression</td>\n",
       "      <td>Pilar Albert, Elvira Mayordomo, Philippe Moser, Sylvain Perifel</td>\n",
       "      <td>0709.2346</td>\n",
       "      <td>0.777091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trellis-Coded Quantization Based on Maximum-Hamming-Distance Binary\\n  Codes</td>\n",
       "      <td>Lorenzo Cappellari</td>\n",
       "      <td>0704.1411</td>\n",
       "      <td>0.776343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This paper was the EMNLP 2019 Best Paper\n",
    "search_papers(title='Specializing Word Embeddings (for Parsing) by Information Bottleneck',\n",
    "              abstract='Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1392,  0.0030,  0.0470,  ...,  0.0641, -0.0163,  0.0636],\n",
      "        [ 0.0227, -0.0014, -0.0056,  ..., -0.0225,  0.0846, -0.0283],\n",
      "        [-0.1004, -0.0774, -0.0014,  ..., -0.0010,  0.0718,  0.0221]])\n",
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "print(embeddings1)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
